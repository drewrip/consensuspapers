\documentclass{article}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{calc}

\begin{document}

\title{An Overview of Distributed Consensus}
\author{Drew Ripberger}
\date{\today}
\tableofcontents{}

\maketitle

\section{Verification and Checking}

\begin{itemize}

	\subsection{Analysis and Checking Frameworks}
	
	%% Verdi
	\item
	\textit{Verdi: A Framework for Implementing and Formally Verifying Distributed Systems} \cite{Verdi}

	Verdi is a framework for practically verifying distributed systems. Often implementations of distributed systems are
	too complex to be exhaustively tested, so, Verdi attempts to choose an appropriate fault model to more effectively enumerate bugs and faults.
	A toolchain is provided to assist in transforming the formal model of the system into implementation.

	%% Teaching Rigorous DS W/ Efficient Model Checking
	\item
	\textit{Teaching Rigorous Distributed Systems with Efficient Model Checking} \cite{MichaelWAET2019}

	While exhaustively determining bugs in a distributed system can be incredibly effective, it can at the same time, be incredibly costly for developers.
	This paper purposes a model that allows students, or developers with fewer resources at their disposal to efficiently verify their systems and visually debug them.
	Also included are methods to reduce the search space for potential faults in the system and to detect errors in realtime.

	%% NEAT
	\item
	\textit{An Analysis of Network-Partitioning Failures in Cloud Systems} \cite{NEAT}

	In an analysis of many popular distributed systems, they found that many, while extensively tested, still have numerous faults present.
	Many of these faults require little to no user input, and are present once a network partition occurs. With this analysis, NEAT is created.
	NEAT is a testing framework that intelligently injects network-partition faults into a distributed system in order to more efficiently and accurately identify potential errors in the implementation.

	\subsection{Reducing Fault Search Space}

	%% SAMC
	\item
	\textit{SAMC: Semantic-Aware Model Checking for Fast Discovery of Deep Bugs in Cloud Systems} \cite{SAMC}

	Model checking is pertinent for ensuring the safety of large cloud systems, however doing so in implementation is incredibly expensive. SAMC proposes policies to simplify the state of model checking.
	It uses systematic methods to reduce the verification necessary, with no randomness. 
	SAMC introduces four novel policies to reduce state-space: local message independence(LMI), crash-message independence(CMI), crash recovery symmetry(CRS), and reboot synchronization symmetry(RSS).

	\item
	\textit{Lineage-driven Fault Injection} \cite{Molly}

	A common approach to detecting faults in a distributed system is to randomly inject failures or malformed messages into the network.
	However, with this approach it is very unlikely that we can detect rarely occurring errors, and moreover it is important to note that this random injection can never guarantee to enumerate all errors in the system.
	The proposed solution is Molly: a fault injection technique that utilizes a lineage based approach to discover bugs based on what possible errors could have affected the correct outcomes of the system.
	
\end{itemize}

\section{Generalizations}

\begin{itemize}

	%% Howard's Generalized Consensus Solution
	\item
	\textit{A Generalised Solution to Distributed Consensus} \cite{HowardGeneralized}

	This paper attempts to simplify the general consensus problem. It looks at the general consensus problem, and considers how it may be simplified in universal terms with respect to immutable state. They look specifically at the Paxos algorithm as an example. 
	It is synonymous with consensus, though, can be incredibly difficult to understand. This generalized solution to consensus hopes to quell some of this confusion.
	In analysis, they find that quorum requirements of many algorithms could in fact be weakened.

	%% Lamport's Generalized Consensus
	\item
	\textit{Generalized Consensus and Paxos} \cite{lamport2005generalized}

	This is a generalized way of representing the consensus problem.
	Lamport boils down the main goals of the Paxos algorithm as a set of mathematical generalizations, that can be proven. He also illustrates that main goals for consensus in terms of command-structure sets.


\end{itemize}

\section{Consensus}

\begin{itemize}

	\subsection{Improvements on Paxos and State Machine Replication}

	%% Fast Paxos
	\item
	\textit{Fast Paxos} \cite{lamport2006fast}

	Fast Paxos is a new variant of Paxos from Leslie Lamport that emphasizes speed of consensus.
	By reducing the quorum size, and implementing a new \textit{fast} round of Paxos, that tests for liveness.
	In Fast Paxos rounds are split into Fast, and Classic rounds. During fast rounds, clients may submit proposals directly to the acceptors, though requiring a larger quorum of acceptors.

	%% FuzzyLog
	\item
	\textit{The FuzzyLog: A Partially Ordered Shared Log} \cite{FuzzyLog}

	Given the cost of maintaining a total order with a shared log, FuzzLog proposes using a partial order in order to cut down on the associated expense.
	In this partially ordered log there exist DAGs of updates, that are uniquely \textit{colored} based on geographic region.
	Resulting replication is much simpler to achieve, as differently \textit{colored} chains are stored and have to be explicitly updated at each replica.


	\subsection{Geo-replication and WANs}

	%% SDPaxos
	\item
	\textit{SDPaxos: Building Efficient Semi-Decentralized Geo-replicated State Machines} \cite{zhao2018sdpaxos}

	Systems attempting geo-replication have run into multiple notorious problems: mainly load imbalance.
	SDPaxos proposes an alternative algorithm that is based on Paxos, that separates consensus into two distinct phases, replicating the commands to the nodes, and enforcing a consistent order on the nodes.
	This is done in an attempt to curb workload imbalance by maintaining optimal one-trip latency in two steps.

	%% Mencius
	\item
	\textit{Mencius: building efficient replicated state machines for WANs} \cite{Mencius}

	Traditional consensus algorithms, like Paxos, are effective in local contexts, but in WANs they often suffer the consequence of geographic separation.
	Often when Paxos, or a version of it, is implemented in a WAN there is a definite increase in network latency, decrease in network throughput and much more prevalent problems with load distribution.
	Mencius however, is the proposed algorithm that attempts to lessen problems traditionally associated with WANs and consensus.
	It does this by partitioning sequences (of commits) across multiple nodes in the network, slowly reducing the load on any one specific participant. Mencius adaptively allows nodes with less load to skip their turns and propose changes.

	%% MDCC
	\item
	\textit{MDCC: Multi-Data Center Consistency} \cite{MDCC}

	MDCC is a commit protocol for geographically separated data centers.
	Given the increased round-trip time for distant datacenters, it becomes imperative to reduce any possible unnecessary messages. MDCC maintains strong consistency while most other similar protocols rely on eventual consistency.
	MDCC's one round-trip commit time is achieved by piggybacking commit state on transaction messages
	and by executing Generalized Paxos in parallel on individual records.
	

	%% Flaw in EPaxos
	\item
	\textit{On the correctness of Egalitarian Paxos} \cite{SutraEPaxos}

	Egalitarian Paxos utilizes an execution graph to order commands in the state machines of individual processes.
	Generally this speeds up latency, as in the most favorable and most common case, only one round trip time is taken to commit the next command.
	Though while the algorithm is fundamentally correct, there is an error that can potentially lead to inconsistency between replicas present in both the Go implementation and the TLA\textsuperscript{+} specification. 

	%% EPaxos
	\item
	\textit{There Is More Consensus in Egalitarian Paxos} \cite{EPaxos}

	EPaxos is a variant of the Paxos algorithm that builds off previous improvements brought on by projects like Mencius and Generalized Paxos.
	It looks to improve load balancing across a wide area network and to improve the network throughput. With EPaxos, a simply majority of replicas need to be non-faulty.
	This is achieved by removing any leader process, which would serve as a bottleneck. Instead participating nodes have choice as to where they submit.
	As a result the network load can be much more evenly distributed. Now no network recovery is needed when a leader process is downed, creating greater availability.

	%% COPS
	\item
	\textit{Don't Settle for Eventual: Scalable Causal Consistency for Wide-area Storage with COPS} \cite{LloydCOPS}

	COPS is a distributed key-value store that implements the newly defined \textit{causal}+, a strong consistency level for WANs.
	In the past many protocols have elected to use a weaker consistency level for the benefit to availability and throughput, but a stronger consistency makes reasoning about the behavior of a system much easier for developers.
	To make wide-area distributed systems more practical and scalable, COPS continually checks causal dependencies in a local cluster before exposing writes.



\end{itemize}

\section{Databases and Implementations}

\begin{itemize}

	%% Spanner
	\item
	\textit{Spanner: Google's Globally-Distributed Database} \cite{Spanner}

	Spanner is a distributed database developed at Google, with the goal of highly available data in different geographic regions.
	The database is sharded into multiple Paxos replicas in order to better horizontally scale. Data in Spanner is automatically resharded to balance load.
	Using their novel \textit{TrueTime} API, Spanner shows how it can practically guarantee consistency on top of availability with its hyper realistic clock.

	%% Calvin
	\item
	\textit{Calvin: fast distributed transactions for partitioned database systems} \cite{Calvin}

	Calvin serves as a replication layer that sits on top of a distributed database, attempting to efficiently and cost effectively allow for distributed transactions and easy scaling. The main goal of Calvin is to make it possible to turn a generic unreplicated database into a fully ACID complaint distributed database.
	To do so, it implements multiple layers for sequencing and ordering transactions into a serial order in a global log. This global log is then used to ensure a proper ordering for each individual partition.

\end{itemize}

\section{The Byzantine Generals Problem}

The fundamental problem of consensus can be represented with the Byzantine Generals Problem \cite{LamportGeneralsProblem}.

\tikzstyle{army} = [rectangle, text centered, minimum height=1.5cm, minimum width=2cm, draw=black]
\tikzstyle{city} = [circle, text centered, minimum width=1cm, draw=black]
\tikzstyle{spy} = [rectangle, text centered, minimum width=1cm, minimum height=.75cm, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]


\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[node distance=1.5cm]
		\node (con) [city] {Constantinople};
		\node (armyA) [army, below left of=con, yshift=-1.5cm, xshift=-1.25cm] {Army A};
		\node (armyB) [army, below right of=con, yshift=-1.5cm, xshift=1.25cm] {Army B};
		\draw [transform canvas={yshift=0.2cm}, arrow] (armyA.east) -- node[anchor=south] {$M_{1}$} (armyB.west);
		\draw [transform canvas={yshift=-0.5cm}, arrow] (armyB.west) -- node[anchor=south] {$M_{2}$} (armyA.east);
		\draw [arrow] (armyA.north) -- node[above, sloped] {Attack} (con);
		\draw [arrow] (armyB.north) -- node[above, sloped] {Attack} (con);
		\end{tikzpicture}

		\caption[title]{Illustration of the Byzantine Generals problem for two armies A \& B}
	\end{center}
\end{figure}

To illustrate the fundemental problem that consensus algorithms are trying to solve, we can imagine a hypothetical situation where two armies $A$ and $B$ are attempting to attack the city of Constantinople.
This is classically called the Byzantine Generals problem.

Imagine that two armies at once are required to attack at once if they would like to successfully take Constantinople, so both $A$ and $B$ need to coordinate if they want wage a successful attack on the city.
Both armies also have the ability to communicate, but only with messengers.
In an attempt to start planning for their siege, army $A$ sends $M_{1}$ to army $B$ with the following message: \\

\textit{Attack Constantinople at 02:00.}

\textit{- Army A} \\

After receiving $M_{1}$, army $B$ agrees to the plan, so to confirm with army $A$ they reply in agreement with $M_{2}$: \\

\textit{Sounds like a plan. Attacking at 02:00.}

\textit{- Army B}


At the surface level everything looks simple here. Army $A$ and $B$ have a simple process on planning out their attack on the city.
However, this is assuming they have uninterrupted communication. 


\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[node distance=1.5cm]
		\node (con) [city] {Constantinople};
		\node (armyA) [army, below left of=con, yshift=-1.5cm, xshift=-1.25cm] {Army A};
		\node (armyB) [army, below right of=con, yshift=-1.5cm, xshift=1.25cm] {Army B};
		\node (spy) [spy, below of=con, yshift=-1cm] {Spy};
		\draw [transform canvas={yshift=0.2cm}, arrow] (armyA.east) -- node[anchor=south] {$M_{1}$} (spy.west);
		\draw [transform canvas={yshift=-0.5cm}, arrow] (spy.west) -- node[anchor=south] {$M_{2}$} (armyA.east);
		\draw [arrow] (armyA.north) -- node[above, sloped] {Attack} (con);
		\draw [arrow] (armyB.north) -- node[above, sloped] {Attack} (con);
		\end{tikzpicture}

		\caption[title]{The Risks in The Byzantine Generals problem}
	\end{center}
\end{figure}


\section{Breakdown of Consensus Algorithms}

	\begin{center}
		\begin{tabular}{|c c c c|}
		\hline
		Algorithm & Date & Local/WAN & Citation \\
		\hline
		\hline
		Raft & 2014 & Local & \cite{Raft}\\
		\hline
		Generalized Paxos & 2005 & Local & \cite{lamport2005generalized}\\
		\hline
		\hline
		EPaxos & 2013 & WAN & \cite{EPaxos}\\
		\hline
		SDPaxos & 2018 & WAN & \cite{zhao2018sdpaxos}\\
		\hline
		Mencius & 2008 & WAN & \cite{Mencius}\\

		\hline
		\end{tabular}
	\end{center}

\section{Consensus over WANs}

\section{Modern Distributed Databases}

\bibliographystyle{acm}
\bibliography{refs}

\end{document}